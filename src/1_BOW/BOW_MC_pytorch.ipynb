{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data는 김도휘 형제님과 김명찬 형제님이 만들어주신 보편지향 기도 데이터를 사용하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## CSV 에서 기도문 읽어오기\n",
    "def read_data(path_to_file):\n",
    "    df = pd.read_csv(path_to_file, dtype=str)\n",
    "    return df\n",
    "\n",
    "df = read_data('../../data/pray456_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/pray456_v3withid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774\n",
      "774\n"
     ]
    }
   ],
   "source": [
    "X = df['content']\n",
    "y = df['label']\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주님, 대림시기를 맞는 교회가 회개와 화해의 생활을 하며 저희에게  오실 아기 예수님을 기쁜 마음으로 맞이할 수 있도록 도와주소서.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_quiz = df['content'].sample(50)\n",
    "y_quiz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_quiz.sort_index().to_csv('../../data/quiz_pray1_sample50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    1\n",
      "Name: label, dtype: object\n",
      "[0 1 2 3 0]\n",
      "[0 1 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(type(y[0]), y[:5])\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "print(integer_encoded[:5])\n",
    "\n",
    "# # one_hot encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), )\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded[:5])\n",
    "\n",
    "# setup y \n",
    "# y = onehot_encoded\n",
    "\n",
    "y= integer_encoded\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 띄어쓰기로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x.split() for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['주님,',\n",
       " '대림시기를',\n",
       " '맞는',\n",
       " '교회가',\n",
       " '회개와',\n",
       " '화해의',\n",
       " '생활을',\n",
       " '하며',\n",
       " '저희에게',\n",
       " '오실',\n",
       " '아기',\n",
       " '예수님을',\n",
       " '기쁜',\n",
       " '마음으로',\n",
       " '맞이할',\n",
       " '수',\n",
       " '있도록',\n",
       " '도와주소서.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고유 토큰 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어마다 고유한 인덱스를 부여하기 위한 dictionary\n",
    "token_to_index = defaultdict(lambda : len(token_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 고유 인덱스를 부여하는 함수\n",
    "def convert_token_to_idx(token_ls):\n",
    "    for tokens in token_ls:\n",
    "        yield [token_to_index[token] for token in tokens]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(convert_token_to_idx(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유 인덱스로 변환될 경우, 원래 어떤 단어였는지 알기 어려우므로,\n",
    "# 인덱스로 변환된 단어를 본래의 단어로 재변환하기 위한 dictionary 생성\n",
    "index_to_token = {val : key for key,val in token_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인덱싱 결과 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주님, 0\n",
      "대림시기를 1\n",
      "맞는 2\n",
      "교회가 3\n",
      "회개와 4\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(token_to_index.items(), key=operator.itemgetter(1))[:5]:\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈(empty) 단어 가방(Bag of Words) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_reviews = len(X)       # 학습용 리뷰의 총 수\n",
    "n_unique_word = len(token_to_index)  # 고유 단어의 갯수 (BOW의 차원의 크기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy를 사용하면 memory error 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = np.zeros((n_train_reviews, n_unique_word), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy 패키지 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 리뷰 수(150,000) x 고유 단어의 수(450,541)의 크기를 갖는 빈 단어가방 생성\n",
    "# bow_data = sps.lil_matrix((n_train_reviews, n_unique_word), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 가방 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tokens in enumerate(X):\n",
    "    for token in tokens:\n",
    "        # i번 째 리뷰에 등장한 단어들을 세서, 고유 번호에 1씩 더해준다.\n",
    "        bow[i, token] += 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619, 3329) (155, 3329) (619,) (155,)\n",
      "[3 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "bow_train, bow_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, random_state=1212)\n",
    "print(bow_train.shape, bow_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(bow_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(bow_test)\n",
    "accuracy = accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7870967741935484\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.95      0.90        38\n",
      "          1       0.84      0.76      0.80        42\n",
      "          2       0.81      0.71      0.75        41\n",
      "          3       0.64      0.74      0.68        34\n",
      "\n",
      "avg / total       0.79      0.79      0.79       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : ',accuracy)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [3 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train), y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset : bow_train, bow_test, y_train, y_test\n",
    "bow_train, y_train, bow_test, y_test = map(\n",
    "    torch.tensor, (bow_train, y_train, bow_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([619, 3329])\n"
     ]
    }
   ],
   "source": [
    "n, c = bow_train.shape\n",
    "print(bow_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(3)\n",
      "tensor(0) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.min(), y_train.max())\n",
    "print(y_test.min(), y_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([3, 2, 3, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = bow_train[0:bs]  # a mini-batch from x\n",
    "yb = y_train[0:bs]\n",
    "xv = bow_test[0:bs]\n",
    "yv = y_test[0:bs]\n",
    "\n",
    "print(xb[:5])\n",
    "print(yb[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4015, grad_fn=<NegBackward>) tensor(0.0938)\n",
      "tensor(1.3805, grad_fn=<NegBackward>) tensor(0.2969)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(3329, 4) / math.sqrt(3329)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n",
    "\n",
    "def nll(pred, gt):\n",
    "    return -pred[range(gt.shape[0]), gt].mean()\n",
    "\n",
    "def accuracy(out, y):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "loss_func = nll\n",
    "\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n",
    "print(loss_func(model(xv), yv), accuracy(model(xv), yv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 tensor(1.3504, grad_fn=<NegBackward>) tensor(0.4475) tensor(1.3518, grad_fn=<NegBackward>) tensor(0.4774)\n",
      "Epoch: 1 tensor(1.3148, grad_fn=<NegBackward>) tensor(0.5784) tensor(1.3234, grad_fn=<NegBackward>) tensor(0.5419)\n",
      "Epoch: 2 tensor(1.2817, grad_fn=<NegBackward>) tensor(0.6672) tensor(1.2965, grad_fn=<NegBackward>) tensor(0.6065)\n",
      "Epoch: 3 tensor(1.2506, grad_fn=<NegBackward>) tensor(0.7270) tensor(1.2711, grad_fn=<NegBackward>) tensor(0.6194)\n",
      "Epoch: 4 tensor(1.2213, grad_fn=<NegBackward>) tensor(0.7431) tensor(1.2471, grad_fn=<NegBackward>) tensor(0.6000)\n",
      "Epoch: 5 tensor(1.1936, grad_fn=<NegBackward>) tensor(0.7431) tensor(1.2243, grad_fn=<NegBackward>) tensor(0.6258)\n",
      "Epoch: 6 tensor(1.1675, grad_fn=<NegBackward>) tensor(0.7512) tensor(1.2028, grad_fn=<NegBackward>) tensor(0.6258)\n",
      "Epoch: 7 tensor(1.1427, grad_fn=<NegBackward>) tensor(0.7641) tensor(1.1824, grad_fn=<NegBackward>) tensor(0.6258)\n",
      "Epoch: 8 tensor(1.1192, grad_fn=<NegBackward>) tensor(0.7658) tensor(1.1632, grad_fn=<NegBackward>) tensor(0.6516)\n",
      "Epoch: 9 tensor(1.0969, grad_fn=<NegBackward>) tensor(0.7738) tensor(1.1449, grad_fn=<NegBackward>) tensor(0.6710)\n",
      "Epoch: 10 tensor(1.0757, grad_fn=<NegBackward>) tensor(0.7722) tensor(1.1275, grad_fn=<NegBackward>) tensor(0.6903)\n",
      "Epoch: 11 tensor(1.0554, grad_fn=<NegBackward>) tensor(0.7738) tensor(1.1110, grad_fn=<NegBackward>) tensor(0.6968)\n",
      "Epoch: 12 tensor(1.0362, grad_fn=<NegBackward>) tensor(0.7771) tensor(1.0953, grad_fn=<NegBackward>) tensor(0.6968)\n",
      "Epoch: 13 tensor(1.0177, grad_fn=<NegBackward>) tensor(0.7819) tensor(1.0803, grad_fn=<NegBackward>) tensor(0.7097)\n",
      "Epoch: 14 tensor(1.0001, grad_fn=<NegBackward>) tensor(0.7900) tensor(1.0661, grad_fn=<NegBackward>) tensor(0.7097)\n",
      "Epoch: 15 tensor(0.9832, grad_fn=<NegBackward>) tensor(0.7964) tensor(1.0525, grad_fn=<NegBackward>) tensor(0.7097)\n",
      "Epoch: 16 tensor(0.9671, grad_fn=<NegBackward>) tensor(0.8013) tensor(1.0395, grad_fn=<NegBackward>) tensor(0.7161)\n",
      "Epoch: 17 tensor(0.9516, grad_fn=<NegBackward>) tensor(0.8061) tensor(1.0271, grad_fn=<NegBackward>) tensor(0.7161)\n",
      "Epoch: 18 tensor(0.9367, grad_fn=<NegBackward>) tensor(0.8126) tensor(1.0152, grad_fn=<NegBackward>) tensor(0.7226)\n",
      "Epoch: 19 tensor(0.9224, grad_fn=<NegBackward>) tensor(0.8126) tensor(1.0038, grad_fn=<NegBackward>) tensor(0.7355)\n",
      "Epoch: 20 tensor(0.9086, grad_fn=<NegBackward>) tensor(0.8158) tensor(0.9929, grad_fn=<NegBackward>) tensor(0.7419)\n",
      "Epoch: 21 tensor(0.8953, grad_fn=<NegBackward>) tensor(0.8271) tensor(0.9824, grad_fn=<NegBackward>) tensor(0.7419)\n",
      "Epoch: 22 tensor(0.8825, grad_fn=<NegBackward>) tensor(0.8304) tensor(0.9724, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 23 tensor(0.8702, grad_fn=<NegBackward>) tensor(0.8336) tensor(0.9628, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 24 tensor(0.8582, grad_fn=<NegBackward>) tensor(0.8368) tensor(0.9535, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 25 tensor(0.8467, grad_fn=<NegBackward>) tensor(0.8368) tensor(0.9446, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 26 tensor(0.8356, grad_fn=<NegBackward>) tensor(0.8368) tensor(0.9360, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 27 tensor(0.8248, grad_fn=<NegBackward>) tensor(0.8384) tensor(0.9277, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 28 tensor(0.8143, grad_fn=<NegBackward>) tensor(0.8401) tensor(0.9197, grad_fn=<NegBackward>) tensor(0.7484)\n",
      "Epoch: 29 tensor(0.8042, grad_fn=<NegBackward>) tensor(0.8465) tensor(0.9120, grad_fn=<NegBackward>) tensor(0.7677)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 30  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ## Feed foward\n",
    "    pred = model(bow_train)\n",
    "    loss = loss_func(pred, y_train)\n",
    "\n",
    "    ## Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weights -= weights.grad * lr\n",
    "        bias -= bias.grad * lr\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    \n",
    "    print(\"Epoch:\", epoch, loss_func(model(bow_train), y_train), accuracy(model(bow_train), y_train), loss_func(model(bow_test), y_test), accuracy(model(bow_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 3, 1, 0, 2, 2, 0, 1, 3, 0, 2, 2, 1, 3, 1, 0, 1, 0, 3, 2, 0, 1, 1,\n",
       "        2, 1, 0, 3, 1, 1, 0, 2, 0, 1, 1, 0, 3, 3, 1, 1, 3, 2, 2, 3, 0, 1, 2, 1,\n",
       "        3, 0, 1, 2, 0, 2, 3, 0, 3, 3, 2, 3, 1, 1, 1, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch CrossEntrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0009,  0.0155, -0.0365, -0.0214],\n",
      "        [ 0.0047, -0.0089, -0.0168,  0.0142],\n",
      "        [-0.0022, -0.0303, -0.0123, -0.0138],\n",
      "        ...,\n",
      "        [-0.0124,  0.0385, -0.0274,  0.0409],\n",
      "        [ 0.0161, -0.0038,  0.0064,  0.0242],\n",
      "        [ 0.0248, -0.0109,  0.0333, -0.0075]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([64, 4])\n",
      "torch.Size([64])\n",
      "1.3926472663879395 0.171875\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(3329, 4) / math.sqrt(3329))\n",
    "        self.bias = nn.Parameter(torch.zeros(4))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "model = Logistic_Regression()\n",
    "\n",
    "with torch.no_grad(): # To see model's paramter without calculating gradient\n",
    "    for p in model.parameters(): \n",
    "        print(p)\n",
    "    print(model(xb).shape)\n",
    "    print(yb.shape)\n",
    "    \n",
    "    print(loss_func(model(xb), yb).item(), accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(model(xb), yb)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.cross_entropy>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 tensor(1.3184, grad_fn=<NllLossBackward>) tensor(0.4814) tensor(1.3244, grad_fn=<NllLossBackward>) tensor(0.4452)\n",
      "Epoch: 1 tensor(1.2842, grad_fn=<NllLossBackward>) tensor(0.6026) tensor(1.2961, grad_fn=<NllLossBackward>) tensor(0.5226)\n",
      "Epoch: 2 tensor(1.2529, grad_fn=<NllLossBackward>) tensor(0.6494) tensor(1.2701, grad_fn=<NllLossBackward>) tensor(0.6065)\n",
      "Epoch: 3 tensor(1.2237, grad_fn=<NllLossBackward>) tensor(0.6850) tensor(1.2460, grad_fn=<NllLossBackward>) tensor(0.6323)\n",
      "Epoch: 4 tensor(1.1962, grad_fn=<NllLossBackward>) tensor(0.7108) tensor(1.2233, grad_fn=<NllLossBackward>) tensor(0.6387)\n",
      "Epoch: 5 tensor(1.1702, grad_fn=<NllLossBackward>) tensor(0.7270) tensor(1.2018, grad_fn=<NllLossBackward>) tensor(0.6645)\n",
      "Epoch: 6 tensor(1.1455, grad_fn=<NllLossBackward>) tensor(0.7447) tensor(1.1815, grad_fn=<NllLossBackward>) tensor(0.6774)\n",
      "Epoch: 7 tensor(1.1221, grad_fn=<NllLossBackward>) tensor(0.7464) tensor(1.1623, grad_fn=<NllLossBackward>) tensor(0.6710)\n",
      "Epoch: 8 tensor(1.0998, grad_fn=<NllLossBackward>) tensor(0.7528) tensor(1.1441, grad_fn=<NllLossBackward>) tensor(0.6839)\n",
      "Epoch: 9 tensor(1.0786, grad_fn=<NllLossBackward>) tensor(0.7609) tensor(1.1268, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 10 tensor(1.0584, grad_fn=<NllLossBackward>) tensor(0.7690) tensor(1.1104, grad_fn=<NllLossBackward>) tensor(0.6839)\n",
      "Epoch: 11 tensor(1.0391, grad_fn=<NllLossBackward>) tensor(0.7738) tensor(1.0948, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 12 tensor(1.0206, grad_fn=<NllLossBackward>) tensor(0.7819) tensor(1.0799, grad_fn=<NllLossBackward>) tensor(0.6903)\n",
      "Epoch: 13 tensor(1.0030, grad_fn=<NllLossBackward>) tensor(0.7868) tensor(1.0658, grad_fn=<NllLossBackward>) tensor(0.6903)\n",
      "Epoch: 14 tensor(0.9861, grad_fn=<NllLossBackward>) tensor(0.7932) tensor(1.0522, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 15 tensor(0.9699, grad_fn=<NllLossBackward>) tensor(0.7997) tensor(1.0393, grad_fn=<NllLossBackward>) tensor(0.7097)\n",
      "Epoch: 16 tensor(0.9544, grad_fn=<NllLossBackward>) tensor(0.8013) tensor(1.0270, grad_fn=<NllLossBackward>) tensor(0.7161)\n",
      "Epoch: 17 tensor(0.9394, grad_fn=<NllLossBackward>) tensor(0.8045) tensor(1.0152, grad_fn=<NllLossBackward>) tensor(0.7161)\n",
      "Epoch: 18 tensor(0.9251, grad_fn=<NllLossBackward>) tensor(0.8061) tensor(1.0039, grad_fn=<NllLossBackward>) tensor(0.7226)\n",
      "Epoch: 19 tensor(0.9112, grad_fn=<NllLossBackward>) tensor(0.8158) tensor(0.9931, grad_fn=<NllLossBackward>) tensor(0.7290)\n",
      "Epoch: 20 tensor(0.8979, grad_fn=<NllLossBackward>) tensor(0.8191) tensor(0.9827, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 21 tensor(0.8851, grad_fn=<NllLossBackward>) tensor(0.8191) tensor(0.9727, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 22 tensor(0.8727, grad_fn=<NllLossBackward>) tensor(0.8207) tensor(0.9632, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 23 tensor(0.8607, grad_fn=<NllLossBackward>) tensor(0.8255) tensor(0.9540, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 24 tensor(0.8491, grad_fn=<NllLossBackward>) tensor(0.8304) tensor(0.9451, grad_fn=<NllLossBackward>) tensor(0.7419)\n",
      "Epoch: 25 tensor(0.8379, grad_fn=<NllLossBackward>) tensor(0.8320) tensor(0.9366, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 26 tensor(0.8270, grad_fn=<NllLossBackward>) tensor(0.8368) tensor(0.9284, grad_fn=<NllLossBackward>) tensor(0.7613)\n",
      "Epoch: 27 tensor(0.8165, grad_fn=<NllLossBackward>) tensor(0.8417) tensor(0.9204, grad_fn=<NllLossBackward>) tensor(0.7677)\n",
      "Epoch: 28 tensor(0.8063, grad_fn=<NllLossBackward>) tensor(0.8417) tensor(0.9128, grad_fn=<NllLossBackward>) tensor(0.7677)\n",
      "Epoch: 29 tensor(0.7964, grad_fn=<NllLossBackward>) tensor(0.8449) tensor(0.9054, grad_fn=<NllLossBackward>) tensor(0.7677)\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(bow_train)\n",
    "        loss = loss_func(pred, y_train)\n",
    "\n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                 p -= p.grad * lr\n",
    "            model.zero_grad()\n",
    "        \n",
    "        print(\"Epoch:\", epoch, loss_func(model(bow_train), y_train), accuracy(model(bow_train), y_train), loss_func(model(bow_test), y_test), accuracy(model(bow_test), y_test))\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch Layers & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0010, -0.0089, -0.0043,  ...,  0.0163, -0.0024, -0.0043],\n",
      "        [-0.0060,  0.0144,  0.0026,  ..., -0.0031,  0.0007,  0.0127],\n",
      "        [ 0.0168, -0.0105,  0.0068,  ..., -0.0145, -0.0073,  0.0084],\n",
      "        [-0.0171, -0.0101,  0.0166,  ...,  0.0014,  0.0117,  0.0157]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0127, -0.0072, -0.0150,  0.0161], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(3329,4)\n",
    "        self.relu = nn.ReLU() # instead of Heaviside step fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(output) # instead of Heaviside step fn\n",
    "        return output\n",
    "\n",
    "model = Perceptron()\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "#     param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 tensor(1.3765, grad_fn=<NllLossBackward>) tensor(0.3554) tensor(1.3783, grad_fn=<NllLossBackward>) tensor(0.3548)\n",
      "Epoch: 1 tensor(1.3618, grad_fn=<NllLossBackward>) tensor(0.4346) tensor(1.3655, grad_fn=<NllLossBackward>) tensor(0.4452)\n",
      "Epoch: 2 tensor(1.3440, grad_fn=<NllLossBackward>) tensor(0.4798) tensor(1.3478, grad_fn=<NllLossBackward>) tensor(0.5097)\n",
      "Epoch: 3 tensor(1.3212, grad_fn=<NllLossBackward>) tensor(0.5816) tensor(1.3274, grad_fn=<NllLossBackward>) tensor(0.5742)\n",
      "Epoch: 4 tensor(1.2935, grad_fn=<NllLossBackward>) tensor(0.6365) tensor(1.3028, grad_fn=<NllLossBackward>) tensor(0.5871)\n",
      "Epoch: 5 tensor(1.2653, grad_fn=<NllLossBackward>) tensor(0.6866) tensor(1.2786, grad_fn=<NllLossBackward>) tensor(0.6258)\n",
      "Epoch: 6 tensor(1.2375, grad_fn=<NllLossBackward>) tensor(0.7141) tensor(1.2565, grad_fn=<NllLossBackward>) tensor(0.6387)\n",
      "Epoch: 7 tensor(1.2111, grad_fn=<NllLossBackward>) tensor(0.7383) tensor(1.2346, grad_fn=<NllLossBackward>) tensor(0.6516)\n",
      "Epoch: 8 tensor(1.1860, grad_fn=<NllLossBackward>) tensor(0.7609) tensor(1.2146, grad_fn=<NllLossBackward>) tensor(0.6903)\n",
      "Epoch: 9 tensor(1.1621, grad_fn=<NllLossBackward>) tensor(0.7658) tensor(1.1951, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 10 tensor(1.1392, grad_fn=<NllLossBackward>) tensor(0.7738) tensor(1.1767, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 11 tensor(1.1175, grad_fn=<NllLossBackward>) tensor(0.7771) tensor(1.1589, grad_fn=<NllLossBackward>) tensor(0.6968)\n",
      "Epoch: 12 tensor(1.0966, grad_fn=<NllLossBackward>) tensor(0.7787) tensor(1.1423, grad_fn=<NllLossBackward>) tensor(0.6903)\n",
      "Epoch: 13 tensor(1.0767, grad_fn=<NllLossBackward>) tensor(0.7803) tensor(1.1262, grad_fn=<NllLossBackward>) tensor(0.7161)\n",
      "Epoch: 14 tensor(1.0577, grad_fn=<NllLossBackward>) tensor(0.7835) tensor(1.1110, grad_fn=<NllLossBackward>) tensor(0.7161)\n",
      "Epoch: 15 tensor(1.0394, grad_fn=<NllLossBackward>) tensor(0.7884) tensor(1.0964, grad_fn=<NllLossBackward>) tensor(0.7226)\n",
      "Epoch: 16 tensor(1.0218, grad_fn=<NllLossBackward>) tensor(0.8045) tensor(1.0825, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 17 tensor(1.0050, grad_fn=<NllLossBackward>) tensor(0.8142) tensor(1.0691, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 18 tensor(0.9888, grad_fn=<NllLossBackward>) tensor(0.8191) tensor(1.0564, grad_fn=<NllLossBackward>) tensor(0.7355)\n",
      "Epoch: 19 tensor(0.9732, grad_fn=<NllLossBackward>) tensor(0.8239) tensor(1.0441, grad_fn=<NllLossBackward>) tensor(0.7419)\n",
      "Epoch: 20 tensor(0.9583, grad_fn=<NllLossBackward>) tensor(0.8223) tensor(1.0325, grad_fn=<NllLossBackward>) tensor(0.7484)\n",
      "Epoch: 21 tensor(0.9439, grad_fn=<NllLossBackward>) tensor(0.8239) tensor(1.0211, grad_fn=<NllLossBackward>) tensor(0.7419)\n",
      "Epoch: 22 tensor(0.9300, grad_fn=<NllLossBackward>) tensor(0.8304) tensor(1.0103, grad_fn=<NllLossBackward>) tensor(0.7548)\n",
      "Epoch: 23 tensor(0.9166, grad_fn=<NllLossBackward>) tensor(0.8320) tensor(0.9999, grad_fn=<NllLossBackward>) tensor(0.7613)\n",
      "Epoch: 24 tensor(0.9037, grad_fn=<NllLossBackward>) tensor(0.8304) tensor(0.9899, grad_fn=<NllLossBackward>) tensor(0.7613)\n",
      "Epoch: 25 tensor(0.8911, grad_fn=<NllLossBackward>) tensor(0.8320) tensor(0.9802, grad_fn=<NllLossBackward>) tensor(0.7677)\n",
      "Epoch: 26 tensor(0.8791, grad_fn=<NllLossBackward>) tensor(0.8336) tensor(0.9710, grad_fn=<NllLossBackward>) tensor(0.7677)\n",
      "Epoch: 27 tensor(0.8674, grad_fn=<NllLossBackward>) tensor(0.8368) tensor(0.9619, grad_fn=<NllLossBackward>) tensor(0.7806)\n",
      "Epoch: 28 tensor(0.8561, grad_fn=<NllLossBackward>) tensor(0.8401) tensor(0.9533, grad_fn=<NllLossBackward>) tensor(0.7806)\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(bow_train)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log\n",
    "    val_loss = criterion(model(bow_test), y_test)\n",
    "    print(\"Epoch:\", epoch, criterion(model(bow_train), y_train), accuracy(model(bow_train), y_train), criterion(model(bow_test), y_test), accuracy(model(bow_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_hidden = nn.Linear(3329,2000)\n",
    "        self.relu = nn.ReLU() # instead of Heaviside step fn\n",
    "        self.fc_output = nn.Linear(2000, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc_hidden(x)\n",
    "        output = self.relu(output) # instead of Heaviside step fn\n",
    "        output = self.fc_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2820e-02, -1.0636e-02,  4.4281e-03,  ..., -4.3550e-03,\n",
      "         -1.4767e-02,  4.0531e-03],\n",
      "        [-7.6278e-03, -8.9340e-03, -1.0001e-02,  ..., -1.4296e-02,\n",
      "         -1.1692e-02,  3.9751e-03],\n",
      "        [ 9.7446e-03, -5.8376e-03, -1.5760e-02,  ...,  5.7479e-04,\n",
      "          9.7562e-03,  1.4891e-02],\n",
      "        ...,\n",
      "        [-2.2288e-03,  1.6520e-02, -1.3872e-02,  ..., -1.4716e-02,\n",
      "         -1.2840e-02, -1.4484e-02],\n",
      "        [ 1.5840e-02,  4.9464e-03,  6.3476e-03,  ...,  1.5330e-02,\n",
      "          9.9402e-04,  1.0017e-02],\n",
      "        [-8.8604e-03,  7.4830e-03,  7.2563e-05,  ...,  9.7049e-03,\n",
      "          1.0375e-03, -5.7526e-04]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0048,  0.0141, -0.0101,  ...,  0.0034,  0.0037,  0.0099],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0139, -0.0140, -0.0155,  ..., -0.0137,  0.0066, -0.0025],\n",
      "        [-0.0038,  0.0140,  0.0033,  ..., -0.0083,  0.0068, -0.0052],\n",
      "        [-0.0169, -0.0215, -0.0017,  ..., -0.0098,  0.0025,  0.0035],\n",
      "        [ 0.0108,  0.0057, -0.0185,  ...,  0.0026, -0.0016, -0.0024]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0123, -0.0074, -0.0188, -0.0178], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MultiLayerPerceptron()\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 tensor(0.8654, grad_fn=<NllLossBackward>) tensor(0.8368) tensor(0.9679, grad_fn=<NllLossBackward>) tensor(0.7548)\n",
      "Epoch: 1 tensor(0.8349, grad_fn=<NllLossBackward>) tensor(0.8417) tensor(0.9433, grad_fn=<NllLossBackward>) tensor(0.7548)\n",
      "Epoch: 2 tensor(0.8050, grad_fn=<NllLossBackward>) tensor(0.8449) tensor(0.9194, grad_fn=<NllLossBackward>) tensor(0.7613)\n",
      "Epoch: 3 tensor(0.7757, grad_fn=<NllLossBackward>) tensor(0.8514) tensor(0.8963, grad_fn=<NllLossBackward>) tensor(0.7613)\n",
      "Epoch: 4 tensor(0.7472, grad_fn=<NllLossBackward>) tensor(0.8562) tensor(0.8741, grad_fn=<NllLossBackward>) tensor(0.7677)\n",
      "Epoch: 5 tensor(0.7196, grad_fn=<NllLossBackward>) tensor(0.8627) tensor(0.8530, grad_fn=<NllLossBackward>) tensor(0.7742)\n",
      "Epoch: 6 tensor(0.6928, grad_fn=<NllLossBackward>) tensor(0.8691) tensor(0.8328, grad_fn=<NllLossBackward>) tensor(0.7742)\n",
      "Epoch: 7 tensor(0.6669, grad_fn=<NllLossBackward>) tensor(0.8740) tensor(0.8137, grad_fn=<NllLossBackward>) tensor(0.7742)\n",
      "Epoch: 8 tensor(0.6420, grad_fn=<NllLossBackward>) tensor(0.8885) tensor(0.7957, grad_fn=<NllLossBackward>) tensor(0.7742)\n",
      "Epoch: 9 tensor(0.6181, grad_fn=<NllLossBackward>) tensor(0.8950) tensor(0.7787, grad_fn=<NllLossBackward>) tensor(0.7806)\n",
      "Epoch: 10 tensor(0.5950, grad_fn=<NllLossBackward>) tensor(0.8982) tensor(0.7627, grad_fn=<NllLossBackward>) tensor(0.7935)\n",
      "Epoch: 11 tensor(0.5729, grad_fn=<NllLossBackward>) tensor(0.9031) tensor(0.7476, grad_fn=<NllLossBackward>) tensor(0.8000)\n",
      "Epoch: 12 tensor(0.5516, grad_fn=<NllLossBackward>) tensor(0.9047) tensor(0.7335, grad_fn=<NllLossBackward>) tensor(0.7935)\n",
      "Epoch: 13 tensor(0.5312, grad_fn=<NllLossBackward>) tensor(0.9063) tensor(0.7202, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 14 tensor(0.5116, grad_fn=<NllLossBackward>) tensor(0.9176) tensor(0.7078, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 15 tensor(0.4928, grad_fn=<NllLossBackward>) tensor(0.9225) tensor(0.6962, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 16 tensor(0.4748, grad_fn=<NllLossBackward>) tensor(0.9257) tensor(0.6853, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 17 tensor(0.4576, grad_fn=<NllLossBackward>) tensor(0.9289) tensor(0.6751, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 18 tensor(0.4410, grad_fn=<NllLossBackward>) tensor(0.9305) tensor(0.6654, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 19 tensor(0.4251, grad_fn=<NllLossBackward>) tensor(0.9354) tensor(0.6565, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 20 tensor(0.4099, grad_fn=<NllLossBackward>) tensor(0.9402) tensor(0.6481, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 21 tensor(0.3953, grad_fn=<NllLossBackward>) tensor(0.9435) tensor(0.6401, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 22 tensor(0.3813, grad_fn=<NllLossBackward>) tensor(0.9515) tensor(0.6328, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 23 tensor(0.3678, grad_fn=<NllLossBackward>) tensor(0.9532) tensor(0.6257, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 24 tensor(0.3549, grad_fn=<NllLossBackward>) tensor(0.9548) tensor(0.6193, grad_fn=<NllLossBackward>) tensor(0.7871)\n",
      "Epoch: 25 tensor(0.3425, grad_fn=<NllLossBackward>) tensor(0.9564) tensor(0.6131, grad_fn=<NllLossBackward>) tensor(0.7935)\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "loss_func = criterion\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(bow_train)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log\n",
    "    val_loss = criterion(model(bow_test), y_test)\n",
    "    print(\"Epoch:\", epoch, loss_func(model(bow_train), y_train), accuracy(model(bow_train), y_train), loss_func(model(bow_test), y_test), accuracy(model(bow_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
