{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 김도휘 형제님과 김명찬 형제님이 만들어주신 보편지향 기도 데이터를 사용하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## CSV 에서 기도문 읽어오기\n",
    "def read_data(path_to_file):\n",
    "    df = pd.read_csv(path_to_file, dtype=str)\n",
    "    return df\n",
    "\n",
    "df = read_data('../../data/pray456_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/pray456_v3withid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774\n",
      "774\n"
     ]
    }
   ],
   "source": [
    "X = df['content']\n",
    "y = df['label']\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주님, 대림시기를 맞는 교회가 회개와 화해의 생활을 하며 저희에게  오실 아기 예수님을 기쁜 마음으로 맞이할 수 있도록 도와주소서.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_quiz = df['content'].sample(50)\n",
    "y_quiz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_quiz.sort_index().to_csv('../../data/quiz_pray1_sample50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y data one_hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    1\n",
      "Name: label, dtype: object\n",
      "[0 1 2 3 0]\n",
      "[0 1 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(type(y[0]), y[:5])\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "print(integer_encoded[:5])\n",
    "\n",
    "# # one_hot encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), )\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded[:5])\n",
    "\n",
    "# setup y \n",
    "# y = onehot_encoded\n",
    "y= integer_encoded\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 띄어쓰기로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x.split() for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['주님,',\n",
       " '대림시기를',\n",
       " '맞는',\n",
       " '교회가',\n",
       " '회개와',\n",
       " '화해의',\n",
       " '생활을',\n",
       " '하며',\n",
       " '저희에게',\n",
       " '오실',\n",
       " '아기',\n",
       " '예수님을',\n",
       " '기쁜',\n",
       " '마음으로',\n",
       " '맞이할',\n",
       " '수',\n",
       " '있도록',\n",
       " '도와주소서.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고유 토큰 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어마다 고유한 인덱스를 부여하기 위한 dictionary\n",
    "token_to_index = defaultdict(lambda : len(token_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 고유 인덱스를 부여하는 함수\n",
    "def convert_token_to_idx(token_ls):\n",
    "    for tokens in token_ls:\n",
    "        yield [token_to_index[token] for token in tokens]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(convert_token_to_idx(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유 인덱스로 변환될 경우, 원래 어떤 단어였는지 알기 어려우므로,\n",
    "# 인덱스로 변환된 단어를 본래의 단어로 재변환하기 위한 dictionary 생성\n",
    "index_to_token = {val : key for key,val in token_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인덱싱 결과 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주님, 0\n",
      "대림시기를 1\n",
      "맞는 2\n",
      "교회가 3\n",
      "회개와 4\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(token_to_index.items(), key=operator.itemgetter(1))[:5]:\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈(empty) 단어 가방(Bag of Words) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_reviews = len(X)       # 학습용 리뷰의 총 수\n",
    "n_unique_word = len(token_to_index)  # 고유 단어의 갯수 (BOW의 차원의 크기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy를 사용하면 memory error 발생 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = np.zeros((n_train_reviews, n_unique_word), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy 패키지 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 리뷰 수(150,000) x 고유 단어의 수(450,541)의 크기를 갖는 빈 단어가방 생성\n",
    "# bow_data = sps.lil_matrix((n_train_reviews, n_unique_word), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 가방 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tokens in enumerate(X):\n",
    "    for token in tokens:\n",
    "        # i번 째 리뷰에 등장한 단어들을 세서, 고유 번호에 1씩 더해준다.\n",
    "        bow[i, token] += 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619, 3329) (155, 3329) (619,) (155,)\n",
      "[3 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "bow_train, bow_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, random_state=1212)\n",
    "print(bow_train.shape, bow_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(bow_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(bow_test)\n",
    "accuracy = accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7870967741935484\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.95      0.90        38\n",
      "          1       0.84      0.76      0.80        42\n",
      "          2       0.81      0.71      0.75        41\n",
      "          3       0.64      0.74      0.68        34\n",
      "\n",
      "avg / total       0.79      0.79      0.79       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : ',accuracy)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset : bow_train, bow_test, y_train, y_test\n",
    "bow_train, y_train, bow_test, y_test = map(\n",
    "    torch.tensor, (bow_train, y_train, bow_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([619, 3329])\n"
     ]
    }
   ],
   "source": [
    "n, c = bow_train.shape\n",
    "print(bow_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(3)\n",
      "tensor(0) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.min(), y_train.max())\n",
    "print(y_test.min(), y_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([3, 2, 3, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = bow_train[0:bs]  # a mini-batch from x\n",
    "yb = y_train[0:bs]\n",
    "xv = bow_test[0:bs]\n",
    "yv = y_test[0:bs]\n",
    "\n",
    "print(xb[:5])\n",
    "print(yb[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3900, grad_fn=<NegBackward>) tensor(0.2188)\n",
      "tensor(1.3963, grad_fn=<NegBackward>) tensor(0.1875)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(3329, 4) / math.sqrt(3329)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n",
    "\n",
    "def nll(pred, gt):\n",
    "    return -pred[range(gt.shape[0]), gt].mean()\n",
    "\n",
    "loss_func = nll\n",
    "\n",
    "def accuracy(out, y):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n",
    "print(loss_func(model(xv), yv), accuracy(model(xv), yv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 tensor(1.0815, grad_fn=<NegBackward>) tensor(0.8372) tensor(1.1885, grad_fn=<NegBackward>) tensor(0.5312)\n",
      "Epoch: 1 tensor(0.9152, grad_fn=<NegBackward>) tensor(0.8605) tensor(1.0386, grad_fn=<NegBackward>) tensor(0.6562)\n",
      "Epoch: 2 tensor(0.7978, grad_fn=<NegBackward>) tensor(0.8837) tensor(0.9409, grad_fn=<NegBackward>) tensor(0.7188)\n",
      "Epoch: 3 tensor(0.7085, grad_fn=<NegBackward>) tensor(0.9070) tensor(0.8722, grad_fn=<NegBackward>) tensor(0.7656)\n",
      "Epoch: 4 tensor(0.6372, grad_fn=<NegBackward>) tensor(0.9070) tensor(0.8213, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 5 tensor(0.5785, grad_fn=<NegBackward>) tensor(0.9302) tensor(0.7822, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 6 tensor(0.5292, grad_fn=<NegBackward>) tensor(0.9302) tensor(0.7512, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 7 tensor(0.4870, grad_fn=<NegBackward>) tensor(0.9302) tensor(0.7261, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 8 tensor(0.4506, grad_fn=<NegBackward>) tensor(0.9535) tensor(0.7055, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 9 tensor(0.4187, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6883, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 10 tensor(0.3907, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6737, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 11 tensor(0.3658, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6612, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 12 tensor(0.3436, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6505, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 13 tensor(0.3237, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6412, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 14 tensor(0.3057, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6330, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 15 tensor(0.2895, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6257, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 16 tensor(0.2747, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6193, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 17 tensor(0.2612, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6136, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 18 tensor(0.2489, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6085, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 19 tensor(0.2375, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6038, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epoch: 20 tensor(0.2271, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.5996, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 21 tensor(0.2174, grad_fn=<NegBackward>) tensor(1.) tensor(0.5958, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 22 tensor(0.2085, grad_fn=<NegBackward>) tensor(1.) tensor(0.5924, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 23 tensor(0.2002, grad_fn=<NegBackward>) tensor(1.) tensor(0.5892, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 24 tensor(0.1925, grad_fn=<NegBackward>) tensor(1.) tensor(0.5863, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 25 tensor(0.1853, grad_fn=<NegBackward>) tensor(1.) tensor(0.5836, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 26 tensor(0.1786, grad_fn=<NegBackward>) tensor(1.) tensor(0.5811, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 27 tensor(0.1723, grad_fn=<NegBackward>) tensor(1.) tensor(0.5788, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 28 tensor(0.1664, grad_fn=<NegBackward>) tensor(1.) tensor(0.5767, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epoch: 29 tensor(0.1608, grad_fn=<NegBackward>) tensor(1.) tensor(0.5747, grad_fn=<NegBackward>) tensor(0.8125)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 30  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        \n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = bow_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        \n",
    "        ## Feed foward\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        ## Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    \n",
    "    print(\"Epoch:\", epoch, loss_func(model(xb), yb), accuracy(model(xb), yb), loss_func(model(xv), yv), accuracy(model(xv), yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 0, 3, 3, 3, 0, 3, 0, 3, 1, 3, 2, 1, 0, 3, 1, 0, 3, 0, 1, 3,\n",
       "        3, 2, 3, 3, 0, 1, 3, 1, 0, 3, 0, 0, 1, 1, 3, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0246, -0.0038,  0.0053, -0.0121],\n",
      "        [ 0.0251,  0.0316,  0.0260, -0.0077],\n",
      "        [ 0.0009, -0.0002, -0.0086, -0.0164],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0057,  0.0010, -0.0057],\n",
      "        [ 0.0131, -0.0044,  0.0278, -0.0196],\n",
      "        [ 0.0123,  0.0283,  0.0307, -0.0147]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "tensor(1.3787) tensor(0.3256)\n"
     ]
    }
   ],
   "source": [
    "## Same model with nn.Module\n",
    "from torch import nn\n",
    "\n",
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(3329, 4) / math.sqrt(3329))\n",
    "        self.bias = nn.Parameter(torch.zeros(4))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return log_softmax(xb @ self.weights + self.bias)\n",
    "\n",
    "model = Logistic_Regression()\n",
    "loss_func = nll\n",
    "\n",
    "with torch.no_grad(): # To see model's paramter without calculating gradient\n",
    "    for p in model.parameters(): \n",
    "        print(p)\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0 tensor(1.0662, grad_fn=<NegBackward>) tensor(0.7907) tensor(1.1656, grad_fn=<NegBackward>) tensor(0.6406)\n",
      "Epochs: 1 tensor(0.9029, grad_fn=<NegBackward>) tensor(0.8605) tensor(1.0187, grad_fn=<NegBackward>) tensor(0.6875)\n",
      "Epochs: 2 tensor(0.7876, grad_fn=<NegBackward>) tensor(0.9070) tensor(0.9231, grad_fn=<NegBackward>) tensor(0.7188)\n",
      "Epochs: 3 tensor(0.6997, grad_fn=<NegBackward>) tensor(0.9070) tensor(0.8562, grad_fn=<NegBackward>) tensor(0.7812)\n",
      "Epochs: 4 tensor(0.6295, grad_fn=<NegBackward>) tensor(0.9070) tensor(0.8069, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 5 tensor(0.5717, grad_fn=<NegBackward>) tensor(0.9302) tensor(0.7690, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 6 tensor(0.5231, grad_fn=<NegBackward>) tensor(0.9302) tensor(0.7390, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 7 tensor(0.4815, grad_fn=<NegBackward>) tensor(0.9535) tensor(0.7149, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 8 tensor(0.4455, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6950, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 9 tensor(0.4141, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6784, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 10 tensor(0.3865, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6644, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 11 tensor(0.3620, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6525, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 12 tensor(0.3401, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6422, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 13 tensor(0.3205, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6333, grad_fn=<NegBackward>) tensor(0.7812)\n",
      "Epochs: 14 tensor(0.3028, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6254, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 15 tensor(0.2868, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6185, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 16 tensor(0.2722, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6124, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 17 tensor(0.2589, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6069, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 18 tensor(0.2468, grad_fn=<NegBackward>) tensor(0.9767) tensor(0.6020, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 19 tensor(0.2356, grad_fn=<NegBackward>) tensor(1.) tensor(0.5976, grad_fn=<NegBackward>) tensor(0.7969)\n",
      "Epochs: 20 tensor(0.2253, grad_fn=<NegBackward>) tensor(1.) tensor(0.5936, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 21 tensor(0.2158, grad_fn=<NegBackward>) tensor(1.) tensor(0.5899, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 22 tensor(0.2070, grad_fn=<NegBackward>) tensor(1.) tensor(0.5866, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 23 tensor(0.1988, grad_fn=<NegBackward>) tensor(1.) tensor(0.5836, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 24 tensor(0.1912, grad_fn=<NegBackward>) tensor(1.) tensor(0.5808, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 25 tensor(0.1841, grad_fn=<NegBackward>) tensor(1.) tensor(0.5782, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 26 tensor(0.1775, grad_fn=<NegBackward>) tensor(1.) tensor(0.5758, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 27 tensor(0.1713, grad_fn=<NegBackward>) tensor(1.) tensor(0.5736, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 28 tensor(0.1654, grad_fn=<NegBackward>) tensor(1.) tensor(0.5716, grad_fn=<NegBackward>) tensor(0.8125)\n",
      "Epochs: 29 tensor(0.1600, grad_fn=<NegBackward>) tensor(1.) tensor(0.5697, grad_fn=<NegBackward>) tensor(0.8125)\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        n_batches = (n - 1) // bs + 1\n",
    "        for i in range(n_batches):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            \n",
    "            xb = bow_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            \n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            #back propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                     p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "        \n",
    "        print(\"Epochs:\", epoch, loss_func(model(xb), yb), accuracy(model(xb), yb), loss_func(model(xv), yv), accuracy(model(xv), yv))\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch CrossEntrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0498, -0.0229,  0.0058, -0.0068],\n",
      "        [ 0.0021,  0.0212, -0.0076,  0.0046],\n",
      "        [ 0.0232, -0.0356,  0.0176,  0.0359],\n",
      "        ...,\n",
      "        [ 0.0418, -0.0022, -0.0249, -0.0178],\n",
      "        [ 0.0153, -0.0133,  0.0092,  0.0018],\n",
      "        [-0.0436,  0.0283, -0.0374,  0.0161]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([43, 4])\n",
      "torch.Size([43])\n",
      "tensor(1.3832) tensor(0.2558)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(3329, 4) / math.sqrt(3329))\n",
    "        self.bias = nn.Parameter(torch.zeros(4))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "model = Logistic_Regression()\n",
    "\n",
    "with torch.no_grad(): # To see model's paramter without calculating gradient\n",
    "    for p in model.parameters(): \n",
    "        print(p)\n",
    "    print(model(xb).shape)\n",
    "    print(yb.shape)\n",
    "    \n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(model(xb), yb)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.cross_entropy>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0 tensor(1.0779, grad_fn=<NllLossBackward>) tensor(0.8140) tensor(1.1801, grad_fn=<NllLossBackward>) tensor(0.6875)\n",
      "Epochs: 1 tensor(0.9139, grad_fn=<NllLossBackward>) tensor(0.8605) tensor(1.0327, grad_fn=<NllLossBackward>) tensor(0.7344)\n",
      "Epochs: 2 tensor(0.7977, grad_fn=<NllLossBackward>) tensor(0.8837) tensor(0.9365, grad_fn=<NllLossBackward>) tensor(0.7188)\n",
      "Epochs: 3 tensor(0.7090, grad_fn=<NllLossBackward>) tensor(0.9070) tensor(0.8690, grad_fn=<NllLossBackward>) tensor(0.7812)\n",
      "Epochs: 4 tensor(0.6381, grad_fn=<NllLossBackward>) tensor(0.9302) tensor(0.8191, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 5 tensor(0.5796, grad_fn=<NllLossBackward>) tensor(0.9302) tensor(0.7807, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 6 tensor(0.5303, grad_fn=<NllLossBackward>) tensor(0.9302) tensor(0.7504, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 7 tensor(0.4881, grad_fn=<NllLossBackward>) tensor(0.9302) tensor(0.7259, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 8 tensor(0.4516, grad_fn=<NllLossBackward>) tensor(0.9302) tensor(0.7058, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 9 tensor(0.4198, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6890, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 10 tensor(0.3917, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6749, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 11 tensor(0.3667, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6628, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 12 tensor(0.3445, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6524, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 13 tensor(0.3246, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6434, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 14 tensor(0.3066, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6355, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 15 tensor(0.2903, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6285, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 16 tensor(0.2755, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6223, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 17 tensor(0.2620, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6168, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 18 tensor(0.2496, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6119, grad_fn=<NllLossBackward>) tensor(0.7969)\n",
      "Epochs: 19 tensor(0.2382, grad_fn=<NllLossBackward>) tensor(0.9767) tensor(0.6074, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 20 tensor(0.2278, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.6034, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 21 tensor(0.2181, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5997, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 22 tensor(0.2091, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5964, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 23 tensor(0.2008, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5933, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 24 tensor(0.1931, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5905, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 25 tensor(0.1859, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5879, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 26 tensor(0.1791, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5855, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 27 tensor(0.1728, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5833, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 28 tensor(0.1669, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5813, grad_fn=<NllLossBackward>) tensor(0.8125)\n",
      "Epochs: 29 tensor(0.1614, grad_fn=<NllLossBackward>) tensor(1.) tensor(0.5794, grad_fn=<NllLossBackward>) tensor(0.8125)\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3385e-02,  8.2254e-03, -6.3626e-03,  ...,  3.2154e-03,\n",
      "         -1.5589e-02, -4.7928e-03],\n",
      "        [-1.3171e-02,  1.7026e-02, -4.5189e-03,  ...,  9.7757e-03,\n",
      "         -3.4634e-04, -1.3953e-02],\n",
      "        [ 9.7065e-03, -1.5890e-02,  9.7550e-03,  ...,  1.2983e-02,\n",
      "         -6.8674e-05,  1.1500e-02],\n",
      "        [ 6.4180e-03, -1.3813e-03,  1.2486e-02,  ..., -6.7461e-03,\n",
      "         -7.7586e-03, -2.1890e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0074, -0.0015, -0.0074, -0.0038], requires_grad=True)\n",
      "tensor(8.1103) tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmc55\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(3329,4)\n",
    "        self.softmax = torch.nn.Softmax() # instead of Heaviside step fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.softmax(x) # instead of Heaviside step fn\n",
    "        return output\n",
    "\n",
    "model = Perceptron()\n",
    "\n",
    "with torch.no_grad(): # To see model's paramter without calculating gradient\n",
    "    for p in model.parameters(): \n",
    "        print(p)\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmc55\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-e6d2b5e09952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss = loss_func(model(xb), yb)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 8.110262870788574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmc55\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-1fc186b09362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {}: train loss: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "criterion = F.cross_entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "model.train()\n",
    "epoch = 20\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(bow_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
