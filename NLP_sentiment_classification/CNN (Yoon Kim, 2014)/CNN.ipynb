{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# data를 읽어옴\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "x_train, y_train = read_txt('../ratings_train.txt')\n",
    "x_test, y_test = read_txt('../ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비어있는 리뷰 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_review(X, Y):\n",
    "    empty_idx_ls = []\n",
    "    \n",
    "    for idx, review in enumerate(X):\n",
    "        if len(review) == 0:\n",
    "            empty_idx_ls.append(idx)\n",
    "    \n",
    "    # idx 값이 큰 것부터 제거 (앞으로 밀리는 것을 방지)\n",
    "    empty_idx_ls = sorted(empty_idx_ls, reverse = True)\n",
    "    \n",
    "    for empty_idx in empty_idx_ls:\n",
    "        del X[empty_idx], Y[empty_idx]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = remove_empty_review(x_train, y_train)\n",
    "x_test, y_test = remove_empty_review(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 인덱싱 (token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 idx 부여\n",
    "def convert_token_to_idx(token_ls):\n",
    "    for tokens in token_ls:\n",
    "        yield [token2idx[token] for token in tokens.split(' ')]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = defaultdict(lambda : len(token2idx)) # token과 index를 매칭시켜주는 딕셔너리\n",
    "pad = token2idx['<PAD>']  # pytorch Variable로 변환하기 위해, 문장의 길이를 맞춰주기 위한 padding \n",
    "\n",
    "x_train = list(convert_token_to_idx(x_train))\n",
    "x_test = list(convert_token_to_idx(x_test))\n",
    "\n",
    "idx2token = {val : key for key,val in token2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인덱싱 결과 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원본 텍스트로 변환 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙..', '진짜', '짜증나네요', '목소리']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2token[x] for x in x_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Variable로 변환하기 위해서는 모든 data의 길이(length)가 동일하여야 한다.\n",
    "# 영화 리뷰는 길이가 제각각이므로, 길이를 맞춰주는 작업을 수행\n",
    "# 짧은 문장에는 padding(공간을 채우기 위해 사용하는 더미)을 추가하고,\n",
    "# 긴 문장은 짤라서 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length를 맞추기 위한 padding\n",
    "def add_padding(token_ls, max_len):\n",
    "    for i, tokens in enumerate(token_ls):\n",
    "        n_token = len(tokens)\n",
    "        \n",
    "        # 길이가 짧으면 padding을 추가\n",
    "        if n_token < max_len:\n",
    "            token_ls[i] += [pad] * (max_len - n_token) # 부족한 만큼 padding을 추가함\n",
    "        \n",
    "        # 길이가 길면, max_len에서 짜름\n",
    "        elif n_token > max_len:\n",
    "            token_ls[i] = tokens[:max_len]\n",
    "    return token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "x_train = add_padding(x_train, max_len)\n",
    "x_test = add_padding(x_test, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2token[x] for x in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 모델 학습을 위해 Data의 type을 Variable 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Variable로 변환\n",
    "def convert_to_long_variable(w2i_ls):\n",
    "    return Variable(torch.LongTensor(w2i_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_long_variable(x_train)\n",
    "x_test = convert_to_long_variable(x_test)\n",
    "\n",
    "y_train = convert_to_long_variable(y_train)\n",
    "y_test = convert_to_long_variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yoon Kim, 2014, Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://datawarrior.files.wordpress.com/2016/10/cnn.png?w=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train된 Embedding은 사용하지 않았습니다.\n",
    "\n",
    "모든 embedding은 랜덤으로 초기화된 상태로 학습을 진행하였습니다. (Rand)\n",
    "\n",
    "hyper-paramter는 자의적인 기준에서 선정하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, pad_index, hid_size, drop_rate, kernel_size_ls, num_filter, n_class):\n",
    "        super(CNN_text, self).__init__()\n",
    "        \n",
    "        self.pad_index = pad_index              # 단어 embedding 과정에서 제외시킬 padding token\n",
    "        self.embed_size = embed_size            # 임베딩 차원의 크기\n",
    "        self.hid_size = hid_size                # 히든 레이어 갯수\n",
    "        self.drop_rate = drop_rate              # 드롭아웃 비율\n",
    "        self.num_filter = num_filter            # 필터의 갯수 \n",
    "        self.kernel_size_ls = kernel_size_ls    # 각기 다른 필터 사이즈가 담긴 리스트\n",
    "        self.num_kernel = len(kernel_size_ls)   # 필터 사이즈의 종류 수\n",
    "        self.n_class = n_class                  # 카테고리 갯수\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=n_words, \n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=self.pad_index\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # kernel size는 (n-gram, embed_size)이다.\n",
    "        # 커널의 열(column)의 크기는 embed_size와 일치하므로, 단어 임베딩 벡터를 모두 커버한다.\n",
    "        # 따라서, n의 row 크기를 갖는 커널은 한번에 n개의 단어를 커버하는 n-gram 커널이라고 볼 수 있다.\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, n_class),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x) # batch_size x max_length x embed_size\n",
    "        embed.unsqueeze_(1)       # batch_size, 1, max_length, embed_size : convolution을 위해 4D로 차원을 조절\n",
    "        \n",
    "        # convolution\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        \n",
    "        # max_pooling\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "            \n",
    "        # dropout\n",
    "        dropouted = [F.dropout(pool, self.drop_rate) for pool in pooled]\n",
    "        \n",
    "        # concatenate\n",
    "        concated = torch.cat(dropouted, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        logit = self.lin(concated)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_words' : len(token2idx),        # 고유한 단어 토큰의 갯수\n",
    "    'embed_size' : 128,                # 임베딩 차원의 크기\n",
    "    'pad_index' : token2idx['<PAD>'],  # 패딩 토큰\n",
    "    'hid_size' : 128,                  # 히든 레이어 갯수\n",
    "    'drop_rate' : 0.5,                 # 드롭아웃 비율          (원문에서는 0.5를 사용)\n",
    "    'kernel_size_ls' : [2,3,4,5],      # 커널 사이즈 리스트        (원문애서는 3,4,5를 사용)\n",
    "    'num_filter' : 32,                 # 각 사이즈 별 커널 갯수 (원문에서는 100을 사용)\n",
    "    'n_class' : 2,                  # 카테고리 갯수\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_text(\n",
       "  (embed): Embedding(450542, 128, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 32, kernel_size=(2, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 32, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 32, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (3): Conv2d(1, 32, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-24-6a32fc092d01>\", line 27, in <module>\n",
      "    scores = model(x_batch)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-19-927ec94f96d9>\", line 37, in forward\n",
      "    conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
      "  File \"<ipython-input-19-927ec94f96d9>\", line 37, in <listcomp>\n",
      "    conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/nn/modules/conv.py\", line 320, in forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 1459, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 1417, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 677, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 723, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/posixpath.py\", line 373, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/posixpath.py\", line 407, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "lr = 0.0003\n",
    "batch_size = 10000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr) # 원문에서는 Adadelta 알고리즘을 사용\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        \n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / y_test.size(0)\n",
    "        loss = criterion(scores, y_test.long())\n",
    "        \n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item()/y_test.size(0), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
