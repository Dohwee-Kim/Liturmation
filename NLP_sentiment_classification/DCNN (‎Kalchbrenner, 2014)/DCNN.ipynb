{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls\n",
    "\n",
    "\n",
    "def convert_word_to_idx(sents):\n",
    "    for sent in sents:\n",
    "        yield [w2i_dict[word] for word in sent.split(' ')]\n",
    "    return\n",
    "\n",
    "\n",
    "def add_padding(sents, max_len):\n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(sent)< max_len:\n",
    "            sents[i] += [pad] * (max_len - len(sent))\n",
    "    \n",
    "        elif len(sent) > max_len:\n",
    "            sents[i] = sent[:max_len]\n",
    "    \n",
    "    return sents\n",
    "\n",
    "\n",
    "def convert_to_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_dict = defaultdict(lambda : len(w2i_dict))\n",
    "pad = w2i_dict['<PAD>']\n",
    "\n",
    "# 데이터 불러오기\n",
    "x_train, y_train = read_txt('ratings_train.txt')\n",
    "x_test, y_test = read_txt('ratings_test.txt')\n",
    "\n",
    "x_train = list(convert_word_to_idx(x_train))\n",
    "x_test = list(convert_word_to_idx(x_test))\n",
    "\n",
    "i2w_dict = {val : key for key, val in w2i_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_variable(add_padding(x_train, 15))\n",
    "x_val = convert_to_variable(add_padding(x_test[:10000],15))\n",
    "x_test = convert_to_variable(add_padding(x_test[10000:],15))\n",
    "\n",
    "y_train = convert_to_variable(y_train).float()\n",
    "y_val = convert_to_variable(y_test[:10000]).float()\n",
    "y_test = convert_to_variable(y_test[10000:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 40000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DynamicKMaxPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CNN_DynamicKMaxPooling, self).__init__()\n",
    "        \n",
    "        self.batch_size   = kwargs['batch_size']\n",
    "        self.n_words      = kwargs['n_words']\n",
    "        self.embed_size   = kwargs['embed_size']\n",
    "        self.n_map_1      = kwargs['n_feature_map_1']\n",
    "        self.n_map_2      = kwargs['n_feature_map_2']\n",
    "        self.k_top        = kwargs['k_top'] # final pooling k\n",
    "        self.L            = kwargs['L']  # number of convolution-layers\n",
    "        self.hid_size     = kwargs['hid_size']\n",
    "        self.n_category   = kwargs['n_category']\n",
    "        self.dropout      = kwargs['dropout']\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        KS_1 = 7 # first convolution filter size (row)\n",
    "        KS_2 = 5 # second filter size (row)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_words, self.embed_size)\n",
    "       \n",
    "        # convolution layers\n",
    "        self.conv_dict = {}\n",
    "        self.conv_dict['conv1'] = nn.Conv2d(1, self.n_map_1, (KS_1, 1), padding=(KS_1-1, 0))\n",
    "        self.conv_dict['conv2'] = nn.Conv2d(self.n_map_1, self.n_map_2, (KS_2, 1), padding=(KS_2-1, 0))\n",
    "        \n",
    "        self.modules = nn.ModuleDict(self.conv_dict)\n",
    "        \n",
    "        \n",
    "        # Fully-connect\n",
    "        self.fully_connect = nn.Sequential(\n",
    "            nn.Linear(self.n_map_2 * self.k_top * self.embed_size//4, self.hid_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            \n",
    "            nn.Linear(self.hid_size, self.n_category),\n",
    "        )\n",
    "        \n",
    "    def dynamic_k_max_pooling(self, x, l):\n",
    "        s = x.size(2)\n",
    "        k_l = round(max(self.k_top, ((self.L-l)/self.L * s)))\n",
    "        index = x.topk(k_l, dim=2)[1].sort(dim=2)[0]\n",
    "        return x.gather(2, index)\n",
    "    \n",
    "    def fold(self, x):\n",
    "        # Embedding column을 2개씩 묶어서 더하는 folding작업을 하는 함수\n",
    "        N, C, S, E = x.size() # [batch_size(1), channel(2), sequence_length, embedding_size]\n",
    "        zero = torch.zeros((N, C, S, E//2))\n",
    "\n",
    "        for c in range(C):\n",
    "            for s in range(S):\n",
    "                i = 0 # i번째 묶음 Folding\n",
    "\n",
    "                for start_idx, end_idx in zip(range(0, E, 2),\n",
    "                                              range(2, E+1, 2)):\n",
    "                    # folding 값 계산\n",
    "                    folded_val = torch.sum(x[0, c, s, start_idx : end_idx])\n",
    "\n",
    "                    # zero 텐서에 결과 누적\n",
    "                    zero[0, c, s, i] += folded_val\n",
    "\n",
    "                    # 다음 묶음으로 넘어감\n",
    "                    i += 1 \n",
    "        return zero\n",
    "    \n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeded = self.embedding(x) # [batch_size, sequence_length, embed_size]\n",
    "        embeded.unsqueeze_(1)  # [batch_size, 1, sequence_length, embed_size]\n",
    "        \n",
    "        # 첫 번째 convolution과 k-max-pooling layer를 통과시킨다.\n",
    "        # convolution\n",
    "        conv1 = self.conv_dict['conv1']\n",
    "        conved1 = conv1(embeded) # [batch_size, n_filter_1, sequence_length + KS_1 -1, embed_size]\n",
    "\n",
    "        # folding\n",
    "        folded1 = self.fold(conved1) # [batch_size, n_filter_1, sequence_length + KS_1 -1, embed_size/2]\n",
    "        \n",
    "        # dynamic-k-max-pooling\n",
    "        pooled1 = self.dynamic_k_max_pooling(folded1, l=1) # [batch_size, n_filter_1, k_max1, embed_size/2]\n",
    "\n",
    "        # nonlinearity\n",
    "        first_output = self.tanh(pooled1) # [batch_size, n_filter_1, k_max1, embed_size/2]\n",
    "        \n",
    "        \n",
    "        # 두 번째 convolution에는 Multiple Feature Map이 적용된다.\n",
    "        # 각각의 필터에 대해, multiple 피처맵을 적용하고, 결과를 합하여 새로운 피처맵 생성\n",
    "        # 해당 연산은 자동적으로 수행됨\n",
    "        conv2 = self.conv_dict['conv2']\n",
    "        conved2 = conv2(first_output)  # [batch_size, n_filter_2, k_max1 + KS_2 -1, embed_size/2]\n",
    "\n",
    "        # Folding\n",
    "        folded2 = self.fold(conved2) # [batch_size, n_filter_2, k_max1 + KS_2 - 1, embed_size/4]\n",
    "        \n",
    "        # second K-max Pooling\n",
    "        pooled2 = self.dynamic_k_max_pooling(folded2, l=2) #[batch_size, n_filter_2, k_top, embed_size/4]\n",
    "        \n",
    "        # nonlinearity\n",
    "        second_output = self.tanh(pooled2) # [batch_size, n_filter_2, k_top, embed_size/4]\n",
    "        \n",
    "        # flatten\n",
    "        flat = second_output.squeeze(0).contiguous().view(self.batch_size,-1) # [batch_size, n_filter_2 * k_top * embed_size/4]\n",
    "        \n",
    "        \n",
    "        logit = self.fully_connect(flat)\n",
    "        \n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(w2i_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'batch_size'    : 100,\n",
    "    'n_words'       : n_words,\n",
    "    'embed_size'    : 48,  # as written in paper\n",
    "    'n_feature_map_1': 6,  # as written in paper\n",
    "    'n_feature_map_2' : 14,# as written in paper\n",
    "    'k_top'         : 4,   # as written in paper\n",
    "    'L'             : 2,   # as written in paper\n",
    "    'hid_size'      : 128,\n",
    "    'n_category'    : 2,   # as written in paper (binary classfication)\n",
    "    'dropout'       : 0.5,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_DynamicKMaxPooling(\n",
       "  (tanh): Tanh()\n",
       "  (embedding): Embedding(450543, 48)\n",
       "  (modules): ModuleDict(\n",
       "    (conv1): Conv2d(1, 6, kernel_size=(7, 1), stride=(1, 1), padding=(6, 0))\n",
       "    (conv2): Conv2d(6, 14, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0))\n",
       "  )\n",
       "  (fully_connect): Sequential(\n",
       "    (0): Linear(in_features=672, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_DynamicKMaxPooling(**kwargs)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lr = 0.001\n",
    "batch_size = 100\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        predict = F.softmax(scores, dim = 1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_val)\n",
    "        predict = F.softmax(scores, dim = 1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_val.long()).sum().item() / 10000\n",
    "        loss = criterion(scores, y_val.long())\n",
    "        \n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item(), acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
